#!/usr/bin/env python3
"""
Script to run the improved NCAA tournament bracket predictor with matchup confidence levels.

This script uses the predictions generated by your trained model and a valid bracket 
structure file (provided by the user). It adds confidence levels for each predicted matchup.
"""

import os
import pandas as pd
import numpy as np
import argparse
import math
import copy

# Import the original function but we'll extend it
from draftpredictor import predict_bracket as original_predict
from draftpredictor import parse_bracket, find_team_in_predictions, matchup_prediction

def calculate_confidence(team1_data, team2_data, seed1, seed2, round_num):
    """
    Calculate a confidence level (0-100%) for a matchup prediction.
    
    The confidence is based on:
    1. Difference in champion probabilities
    2. Difference in key metrics (SRS, Win_Pct, etc.)
    3. Seed differential
    4. Round number (later rounds have more uncertainty)
    """
    # Extract key metrics if they exist
    key_metrics = ['SRS', 'Win_Pct', 'SOS', 'Point_Differential', 'Champion_Probability']
    
    # Get probability values
    team1_prob = team1_data.get('Champion_Probability', 0.0)
    team2_prob = team2_data.get('Champion_Probability', 0.0)
    
    # Base confidence on probability ratio
    if team1_prob > team2_prob:
        ratio = team1_prob / max(team2_prob, 0.0001)  # Avoid division by zero
        stronger_seed = seed1
        weaker_seed = seed2
    else:
        ratio = team2_prob / max(team1_prob, 0.0001)  # Avoid division by zero
        stronger_seed = seed2
        weaker_seed = seed1
    
    # Cap the ratio to avoid extreme values
    ratio = min(ratio, 50.0)
    
    # Convert ratio to a base confidence (0-100)
    base_confidence = math.tanh(math.log(ratio) * 0.5) * 50 + 50
    
    # Adjust for seed differential
    seed_diff = abs(seed1 - seed2)
    seed_factor = 1.0 - (0.03 * seed_diff)  # Each seed difference reduces confidence by 3%
    seed_factor = max(0.7, seed_factor)  # Don't reduce by more than 30%
    
    # Upset penalty - reduce confidence for predicted upsets
    upset_penalty = 1.0
    if (seed1 < seed2 and team2_prob > team1_prob) or (seed2 < seed1 and team1_prob > team2_prob):
        upset_penalty = 0.8  # 20% confidence reduction for upset picks
    
    # Round adjustment - later rounds have more uncertainty
    round_factor = 1.0 - (round_num * 0.03)  # Each round reduces confidence by 3%
    round_factor = max(0.8, round_factor)  # Don't reduce by more than 20%
    
    # Calculate final confidence
    confidence = base_confidence * seed_factor * round_factor * upset_penalty
    
    # Scale to 0-100% and round to nearest percent
    confidence = min(round(confidence), 99)  # Cap at 99% - nothing is 100% certain!
    
    return confidence

def predict_bracket_with_confidence(predictions_file, bracket_file, output_file):
    """
    Extended version of predict_bracket that includes confidence levels for each matchup.
    """
    try:
        predictions_df = pd.read_csv(predictions_file)
        print(f"Loaded predictions for {len(predictions_df)} teams")
    except Exception as e:
        print(f"Error loading predictions file: {e}")
        return

    try:
        regions = parse_bracket(bracket_file)
        print(f"Parsed bracket with {len(regions)} regions")
    except Exception as e:
        print(f"Error parsing bracket file: {e}")
        return

    # We'll create our own simulation that mirrors the original but adds confidence
    results = {
        "Round of 64": [],
        "Round of 32": [],
        "Sweet 16": [],
        "Elite 8": [],
        "Final 4": [],
        "Championship": [],
        "Champion": []
    }
    
    # -----------------------------
    # ROUND OF 64
    # -----------------------------
    round_of_32 = {}
    
    for region_name, matchups in regions.items():
        round_of_32[region_name] = []
        
        # For each pairing in the Round of 64
        for matchup in matchups:
            team1, seed1 = matchup["team1"], matchup["seed1"]
            team2, seed2 = matchup["team2"], matchup["seed2"]
            
            # Lookup or fallback for each team
            team1_data = find_team_in_predictions(team1, predictions_df)
            team2_data = find_team_in_predictions(team2, predictions_df)
            if team1_data is None:
                print(f"Warning: {team1} not found in predictions. Using seed-based fallback.")
                team1_data = pd.Series({
                    'School': team1,
                    'SRS': max(20 - seed1, 0),
                    'Win_Pct': max(1.0 - (seed1 * 0.05), 0.5),
                    'Champion_Probability': max(0.16 - (seed1 * 0.01), 0.001)
                })
            if team2_data is None:
                print(f"Warning: {team2} not found in predictions. Using seed-based fallback.")
                team2_data = pd.Series({
                    'School': team2,
                    'SRS': max(20 - seed2, 0),
                    'Win_Pct': max(1.0 - (seed2 * 0.05), 0.5),
                    'Champion_Probability': max(0.16 - (seed2 * 0.01), 0.001)
                })
            
            # Determine winner
            winner, winner_seed, winner_prob = matchup_prediction(team1_data, team2_data,
                                                                seed1, seed2, round_num=1)
            
            # Calculate confidence level
            confidence = calculate_confidence(team1_data, team2_data, seed1, seed2, round_num=1)
            
            # Store Round of 64 result
            results["Round of 64"].append({
                "region": region_name,
                "team1": team1, "seed1": seed1,
                "team2": team2, "seed2": seed2,
                "winner": winner,
                "winner_seed": winner_seed,
                "winner_prob": winner_prob,
                "confidence": confidence
            })
            
            # Winner advances to next round in the same region
            round_of_32[region_name].append({
                "team": winner, 
                "seed": winner_seed, 
                "prob": winner_prob
            })
    
    # -----------------------------
    # ROUND OF 32
    # -----------------------------
    sweet_16 = {}
    
    for region_name, winners in round_of_32.items():
        sweet_16[region_name] = []
        
        # Pair them up 2 by 2
        for i in range(0, len(winners), 2):
            if i + 1 < len(winners):
                team1, seed1 = winners[i]["team"], winners[i]["seed"]
                team2, seed2 = winners[i+1]["team"], winners[i+1]["seed"]
                
                # Lookup or fallback for each team
                team1_data = find_team_in_predictions(team1, predictions_df)
                team2_data = find_team_in_predictions(team2, predictions_df)
                if team1_data is None:
                    team1_data = pd.Series({
                        'School': team1,
                        'SRS': max(20 - seed1, 0),
                        'Win_Pct': max(1.0 - (seed1 * 0.05), 0.5),
                        'Champion_Probability': max(0.16 - (seed1 * 0.01), 0.001)
                    })
                if team2_data is None:
                    team2_data = pd.Series({
                        'School': team2,
                        'SRS': max(20 - seed2, 0),
                        'Win_Pct': max(1.0 - (seed2 * 0.05), 0.5),
                        'Champion_Probability': max(0.16 - (seed2 * 0.01), 0.001)
                    })
                
                # Determine winner
                winner, winner_seed, winner_prob = matchup_prediction(team1_data, team2_data,
                                                                     seed1, seed2, round_num=2)
                
                # Calculate confidence level
                confidence = calculate_confidence(team1_data, team2_data, seed1, seed2, round_num=2)
                
                # Store Round of 32 result
                results["Round of 32"].append({
                    "region": region_name,
                    "team1": team1, "seed1": seed1,
                    "team2": team2, "seed2": seed2,
                    "winner": winner,
                    "winner_seed": winner_seed,
                    "winner_prob": winner_prob,
                    "confidence": confidence
                })
                
                # Winner advances
                sweet_16[region_name].append({
                    "team": winner, 
                    "seed": winner_seed,
                    "prob": winner_prob
                })
    
    # -----------------------------
    # SWEET 16
    # -----------------------------
    elite_8 = {}
    
    for region_name, winners in sweet_16.items():
        elite_8[region_name] = []
        
        for i in range(0, len(winners), 2):
            if i + 1 < len(winners):
                team1, seed1 = winners[i]["team"], winners[i]["seed"]
                team2, seed2 = winners[i+1]["team"], winners[i+1]["seed"]
                
                team1_data = find_team_in_predictions(team1, predictions_df)
                team2_data = find_team_in_predictions(team2, predictions_df)
                if team1_data is None:
                    team1_data = pd.Series({
                        'School': team1,
                        'SRS': max(20 - seed1, 0),
                        'Win_Pct': max(1.0 - (seed1 * 0.05), 0.5),
                        'Champion_Probability': max(0.16 - (seed1 * 0.01), 0.001)
                    })
                if team2_data is None:
                    team2_data = pd.Series({
                        'School': team2,
                        'SRS': max(20 - seed2, 0),
                        'Win_Pct': max(1.0 - (seed2 * 0.05), 0.5),
                        'Champion_Probability': max(0.16 - (seed2 * 0.01), 0.001)
                    })
                
                winner, winner_seed, winner_prob = matchup_prediction(team1_data, team2_data,
                                                                    seed1, seed2, round_num=3)
                
                # Calculate confidence level
                confidence = calculate_confidence(team1_data, team2_data, seed1, seed2, round_num=3)
                
                results["Sweet 16"].append({
                    "region": region_name,
                    "team1": team1, "seed1": seed1,
                    "team2": team2, "seed2": seed2,
                    "winner": winner,
                    "winner_seed": winner_seed,
                    "winner_prob": winner_prob,
                    "confidence": confidence
                })
                
                elite_8[region_name].append({
                    "team": winner, 
                    "seed": winner_seed,
                    "prob": winner_prob
                })
    
    # -----------------------------
    # ELITE 8
    # -----------------------------
    final_4 = []
    region_order = list(elite_8.keys())
    
    for region_name in region_order:
        winners = elite_8[region_name]
        if len(winners) >= 2:
            team1, seed1 = winners[0]["team"], winners[0]["seed"]
            team2, seed2 = winners[1]["team"], winners[1]["seed"]
            
            team1_data = find_team_in_predictions(team1, predictions_df)
            team2_data = find_team_in_predictions(team2, predictions_df)
            if team1_data is None:
                team1_data = pd.Series({
                    'School': team1,
                    'SRS': max(20 - seed1, 0),
                    'Win_Pct': max(1.0 - (seed1 * 0.05), 0.5),
                    'Champion_Probability': max(0.16 - (seed1 * 0.01), 0.001)
                })
            if team2_data is None:
                team2_data = pd.Series({
                    'School': team2,
                    'SRS': max(20 - seed2, 0),
                    'Win_Pct': max(1.0 - (seed2 * 0.05), 0.5),
                    'Champion_Probability': max(0.16 - (seed2 * 0.01), 0.001)
                })
            
            winner, winner_seed, winner_prob = matchup_prediction(team1_data, team2_data,
                                                                seed1, seed2, round_num=4)
            
            # Calculate confidence level
            confidence = calculate_confidence(team1_data, team2_data, seed1, seed2, round_num=4)
            
            results["Elite 8"].append({
                "region": region_name,
                "team1": team1, "seed1": seed1,
                "team2": team2, "seed2": seed2,
                "winner": winner,
                "winner_seed": winner_seed,
                "winner_prob": winner_prob,
                "confidence": confidence
            })
            
            final_4.append({
                "team": winner, 
                "seed": winner_seed,
                "prob": winner_prob,
                "region": region_name
            })
    
    # -----------------------------
    # FINAL FOUR
    # -----------------------------
    championship = []
    
    for i in range(0, len(final_4), 2):
        if i + 1 < len(final_4):
            team1, seed1 = final_4[i]["team"], final_4[i]["seed"]
            team2, seed2 = final_4[i+1]["team"], final_4[i+1]["seed"]
            region1 = final_4[i]["region"]
            region2 = final_4[i+1]["region"]
            
            team1_data = find_team_in_predictions(team1, predictions_df)
            team2_data = find_team_in_predictions(team2, predictions_df)
            if team1_data is None:
                team1_data = pd.Series({
                    'School': team1,
                    'SRS': max(20 - seed1, 0),
                    'Win_Pct': max(1.0 - (seed1 * 0.05), 0.5),
                    'Champion_Probability': max(0.16 - (seed1 * 0.01), 0.001)
                })
            if team2_data is None:
                team2_data = pd.Series({
                    'School': team2,
                    'SRS': max(20 - seed2, 0),
                    'Win_Pct': max(1.0 - (seed2 * 0.05), 0.5),
                    'Champion_Probability': max(0.16 - (seed2 * 0.01), 0.001)
                })
            
            winner, winner_seed, winner_prob = matchup_prediction(team1_data, team2_data,
                                                                seed1, seed2, round_num=5)
            
            # Calculate confidence level
            confidence = calculate_confidence(team1_data, team2_data, seed1, seed2, round_num=5)
            
            matchup_text = f"{region1} vs {region2}"
            results["Final 4"].append({
                "matchup": matchup_text,
                "team1": team1, "seed1": seed1,
                "team2": team2, "seed2": seed2,
                "winner": winner,
                "winner_seed": winner_seed,
                "winner_prob": winner_prob,
                "confidence": confidence,
                "region": matchup_text
            })
            
            championship.append({
                "team": winner, 
                "seed": winner_seed,
                "prob": winner_prob
            })
    
    # -----------------------------
    # CHAMPIONSHIP
    # -----------------------------
    if len(championship) >= 2:
        team1, seed1 = championship[0]["team"], championship[0]["seed"]
        team2, seed2 = championship[1]["team"], championship[1]["seed"]
        
        team1_data = find_team_in_predictions(team1, predictions_df)
        team2_data = find_team_in_predictions(team2, predictions_df)
        if team1_data is None:
            team1_data = pd.Series({
                'School': team1,
                'SRS': max(20 - seed1, 0),
                'Win_Pct': max(1.0 - (seed1 * 0.05), 0.5),
                'Champion_Probability': max(0.16 - (seed1 * 0.01), 0.001)
            })
        if team2_data is None:
            team2_data = pd.Series({
                'School': team2,
                'SRS': max(20 - seed2, 0),
                'Win_Pct': max(1.0 - (seed2 * 0.05), 0.5),
                'Champion_Probability': max(0.16 - (seed2 * 0.01), 0.001)
            })
        
        winner, winner_seed, winner_prob = matchup_prediction(team1_data, team2_data,
                                                            seed1, seed2, round_num=6)
        
        # Calculate confidence level
        confidence = calculate_confidence(team1_data, team2_data, seed1, seed2, round_num=6)
        
        results["Championship"].append({
            "team1": team1, "seed1": seed1,
            "team2": team2, "seed2": seed2,
            "winner": winner,
            "winner_seed": winner_seed,
            "winner_prob": winner_prob,
            "confidence": confidence,
            "region": "Championship"
        })
        
        results["Champion"].append({
            "team": winner,
            "seed": winner_seed,
            "probability": winner_prob,
            "confidence": confidence
        })
    
    # -----------------------------
    # Print results with confidence levels
    # -----------------------------
    print("\n===== TOURNAMENT PREDICTIONS WITH CONFIDENCE =====\n")
    
    print("ROUND OF 64:")
    for matchup in results["Round of 64"]:
        print(f"{matchup['region']}: ({matchup['seed1']}) {matchup['team1']} vs ({matchup['seed2']}) {matchup['team2']} → ({matchup['winner_seed']}) {matchup['winner']} [Confidence: {matchup['confidence']}%]")
    
    print("\nROUND OF 32:")
    for matchup in results["Round of 32"]:
        print(f"{matchup['region']}: ({matchup['seed1']}) {matchup['team1']} vs ({matchup['seed2']}) {matchup['team2']} → ({matchup['winner_seed']}) {matchup['winner']} [Confidence: {matchup['confidence']}%]")
    
    print("\nSWEET 16:")
    for matchup in results["Sweet 16"]:
        print(f"{matchup['region']}: ({matchup['seed1']}) {matchup['team1']} vs ({matchup['seed2']}) {matchup['team2']} → ({matchup['winner_seed']}) {matchup['winner']} [Confidence: {matchup['confidence']}%]")
    
    print("\nELITE 8:")
    for matchup in results["Elite 8"]:
        print(f"{matchup['region']}: ({matchup['seed1']}) {matchup['team1']} vs ({matchup['seed2']}) {matchup['team2']} → ({matchup['winner_seed']}) {matchup['winner']} [Confidence: {matchup['confidence']}%]")
    
    print("\nFINAL FOUR:")
    for matchup in results["Final 4"]:
        print(f"{matchup['matchup']}: ({matchup['seed1']}) {matchup['team1']} vs ({matchup['seed2']}) {matchup['team2']} → ({matchup['winner_seed']}) {matchup['winner']} [Confidence: {matchup['confidence']}%]")
    
    print("\nCHAMPIONSHIP:")
    for matchup in results["Championship"]:
        print(f"({matchup['seed1']}) {matchup['team1']} vs ({matchup['seed2']}) {matchup['team2']} → ({matchup['winner_seed']}) {matchup['winner']} [Confidence: {matchup['confidence']}%]")
    
    if results["Champion"]:
        champion = results["Champion"][0]
        print("\nCHAMPION:")
        print(f"({champion['seed']}) {champion['team']} - Championship Probability: {champion['probability']:.4f} [Confidence: {champion['confidence']}%]")
    
    # Export bracket to CSV
    all_picks = []
    for round_name, matchups in results.items():
        for matchup in matchups:
            round_data = {
                "Round": round_name,
                "Winner": matchup.get("winner", ""),
                "Winner_Seed": matchup.get("winner_seed", ""),
                "Team1": matchup.get("team1", ""),
                "Seed1": matchup.get("seed1", ""),
                "Team2": matchup.get("team2", ""),
                "Seed2": matchup.get("seed2", ""),
                "Region": matchup.get("region", matchup.get("matchup", "Championship")),
                "Confidence": matchup.get("confidence", "")
            }
            all_picks.append(round_data)
    
    picks_df = pd.DataFrame(all_picks)
    picks_df.to_csv(output_file, index=False)
    print(f"\nExported bracket with confidence levels to {output_file}")

def print_champion_probabilities(predictions_file):
    """Print all teams sorted by their championship probability."""
    try:
        predictions_df = pd.read_csv(predictions_file)
        
        # Check if 'Champion_Probability' column exists
        if 'Champion_Probability' not in predictions_df.columns:
            print("Error: Champion_Probability column not found in predictions file.")
            return
            
        # Sort teams by champion probability in descending order
        sorted_teams = predictions_df.sort_values('Champion_Probability', ascending=False)
        
        # Print the top teams and their probabilities
        print("\n===== CHAMPIONSHIP PROBABILITIES =====\n")
        print("Rank  Team                              Probability")
        print("--------------------------------------------------")
        
        for i, (_, team) in enumerate(sorted_teams.iterrows(), 1):
            team_name = team['School']
            probability = team['Champion_Probability']
            print(f"{i:4d}  {team_name:<35s} {probability:.4f}")
            
            # Show all teams
            if i >= 50 and i % 50 == 0:
                response = input("\nPress Enter to see more teams, or type 'q' to quit: ")
                if response.lower() == 'q':
                    print("... (remaining teams not shown)")
                    break
                
    except Exception as e:
        print(f"Error printing champion probabilities: {e}")

def main():
    """Run the bracket predictor using actual prediction and bracket data."""
    parser = argparse.ArgumentParser(description='NCAA Tournament Bracket Predictor')
    parser.add_argument('--predictions', default='archive/2025_champion_predictions.csv',
                        help='Path to the predictions CSV file (generated by training)')
    parser.add_argument('--bracket', default='bracket_2025.txt',
                        help='Path to the bracket structure file (must contain actual bracket data)')
    parser.add_argument('--output', default='filled_bracket_2025.csv',
                        help='Output file for the filled bracket')
    parser.add_argument('--print-probs', action='store_true',
                        help='Print all championship probabilities before running prediction')
    parser.add_argument('--confidence', action='store_true',
                        help='Include confidence levels for each matchup prediction')
    
    args = parser.parse_args()
    
    # Verify that the predictions file exists.
    if not os.path.exists(args.predictions):
        print(f"Error: Predictions file '{args.predictions}' does not exist.")
        print("Please run the training script to generate the predictions file.")
        return
    
    # Verify that the bracket file exists.
    if not os.path.exists(args.bracket):
        print(f"Error: Bracket file '{args.bracket}' does not exist.")
        print("Please provide a valid bracket structure file.")
        return
    
    # Print championship probabilities if requested
    if args.print_probs:
        print_champion_probabilities(args.predictions)
        choice = input("\nContinue with bracket prediction? (y/n): ")
        if choice.lower() != 'y':
            return

    print("\nRunning NCAA tournament bracket predictor using actual data...")
    
    if args.confidence:
        predict_bracket_with_confidence(args.predictions, args.bracket, args.output)
    else:
        original_predict(args.predictions, args.bracket, args.output)
        
    print("\nBracket prediction completed!")
    print(f"Check '{args.output}' for the filled bracket results.")

if __name__ == "__main__":
    main()